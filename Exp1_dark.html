<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Exp1 - Dark Render</title>
<style>
body { background: #0b0b0b; color: #e6e6e6; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial; line-height:1.6; padding: 24px; }
h1,h2,h3 { color: #ffffff; }
.container { max-width: 900px; margin: auto; }
.card { background: #0f0f10; border: 1px solid #222; padding: 18px; border-radius: 8px; margin-bottom: 18px; }
pre { background: #090909; color: #e6e6e6; padding: 12px; overflow-x: auto; border-radius:6px; border: 1px solid #222; }
code { color: #d6f0ff; }
a { color: #79b8ff; }
.badge { display:inline-block; vertical-align:middle; }
</style>
</head>
<body>
<div class="container">
  <a class="badge" href="https://colab.research.google.com/github/Amitabh-Phule/GenAi/blob/main/Exp1.ipynb" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>
  <h1>Exp1.ipynb â€” Dark Render</h1>
  <div class="card">
    <h2>Objectives</h2>
    <ol>
      <li>Understand the concept of prompting in LLMs</li>
      <li>Implement zero-shot, one-shot, and few-shot prompting</li>
      <li>Analyze the effectiveness of prompt engineering techniques</li>
    </ol>
  </div>

  <div class="card">
    <h2>Install</h2>
    <pre><code>pip install openai</code></pre>
    <p><em>Note:</em> original notebook captured output showing the package was already installed.</p>
  </div>

  <div class="card">
    <h2>Initialize LLM Client</h2>
    <pre><code>from openai import OpenAI

client = OpenAI(api_key="")</code></pre>
  </div>

  <div class="card">
    <h2>LLM Request Function</h2>
    <pre><code>def ask_llm(prompt: str, temperature: float = 0.7) -> str:
    """
    Sends a prompt to the GenAI LLM and returns the response.

    Args:
        prompt (str): The user prompt or one-shot prompt.
        temperature (float): Controls creativity of the model (default 0.7)

    Returns:
        str: Model's text response
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature
    )
    return response.choices[0].message.content</code></pre>
  </div>

  <div class="card">
    <h2>Zero-Shot Prompting (example)</h2>
    <pre><code>print("=== Zero-Shot Prompting ===")
zero_prompt = input("Enter your zero-shot prompt for the AI: ")
zero_result = ask_llm(zero_prompt)
print("\nZero-Shot AI Response:\n", zero_result)</code></pre>
    <p>Captured run in the notebook asked: "who is pm of india?" and the model responded mentioning Narendra Modi (as of Oct 2023).</p>
  </div>

  <div class="card">
    <h2>One-Shot Prompting (example)</h2>
    <pre><code># Define the example for one-shot
example_input = "Translate 'Hello' to Spanish."
example_output = "Hola"

# User input for one-shot task
user_input = input("Enter your text to process (one-shot style): ")

# Construct one-shot prompt
one_shot_prompt = f"""Example:
Input: {example_input}
Output: {example_output}

Now, process this:
Input: {user_input}
Output:"""

# Call the existing function
one_shot_result = ask_llm(one_shot_prompt)

# Print the AI response
print("\nOne-Shot AI Response:\n", one_shot_result)</code></pre>
    <p>Captured run produced an example addition: "5 + 7 = 12" for the user input.</p>
  </div>

  <footer style="margin-top:24px; color:#9aa5b1; font-size:13px;">
    This is a dark-rendered HTML export of Exp1.ipynb created to provide a dark preview on GitHub. To open the original notebook in Colab, click the badge at the top.
  </footer>
</div>
</body>
</html>